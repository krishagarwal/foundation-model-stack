# TRITON_PRINT_AUTOTUNING=1 CUDA_VISIBLE_DEVICES=1 /home/intern/miniconda3/envs/myenv/bin/python /home/intern/qfms/scripts/benchmark_inference.py --device_type cuda --tokenizer /data/intern/llama2-7b/tokenizer.model --variant 7b --quant_dtype int8 --rotate --skip_correctness_check --skip_nokvcache_runs --skip_e2e_runs --skip_eager_runs --compile_mode reduce-overhead
# CUDA_VISIBLE_DEVICES=1 /home/intern/miniconda3/envs/myenv/bin/python /home/intern/qfms/scripts/benchmark_inference.py --device_type cuda --tokenizer /data/intern/llama2-7b/tokenizer.model --variant 7b --quant_dtype int8 --rotate --skip_nokvcache_runs --skip_e2e_runs --skip_eager_runs --compile_mode reduce-overhead

# CUDA_VISIBLE_DEVICES=1 /home/intern/miniconda3/envs/myenv/bin/python /home/intern/qfms/scripts/benchmark_inference.py --device_type cuda --tokenizer /data/intern/llama2-7b/tokenizer.model --variant 7b --quant_dtype int8 --rotate --skip_nokvcache_runs --skip_e2e_runs --skip_eager_runs

# CUDA_VISIBLE_DEVICES=1 /home/intern/miniconda3/envs/myenv/bin/python /home/intern/qfms/scripts/inference.py --device_type cuda --model_path /data/intern/llama2-7b_q_int8_rot/ --tokenizer /data/intern/llama2-7b/tokenizer.model --variant 7b --quant_dtype int8 --rotate
# CUDA_VISIBLE_DEVICES=1 /home/intern/miniconda3/envs/myenv/bin/python /home/intern/qfms/scripts/inference.py --device_type cuda --model_path /data/intern/llama2-7b_q_int8_rot/ --tokenizer /data/intern/llama2-7b/tokenizer.model --variant 7b --quant_dtype int8 --rotate --compile
# CUDA_VISIBLE_DEVICES=1 /home/intern/miniconda3/envs/myenv/bin/python /home/intern/qfms/scripts/inference.py --device_type cuda --model_path /data/intern/llama2-7b/ --tokenizer /data/intern/llama2-7b/tokenizer.model --variant 7b --model_source hf
# /home/intern/miniconda3/envs/myenv/bin/python ./fms/utils/csrc/marlin/old/marlin_example.py
# CUDA_VISIBLE_DEVICES=1 /home/intern/miniconda3/envs/myenv/bin/python /home/intern/qfms/scripts/inference.py --device_type cuda --model_path /data/intern/llama2-7b_q_int8_rot/ --tokenizer /data/intern/llama2-7b/tokenizer.model --variant 7b --quant_dtype int8 --rotate --compile --compile_mode reduce-overhead
# TRITON_PRINT_AUTOTUNING=1 CUDA_VISIBLE_DEVICES=1 /home/intern/miniconda3/envs/test/bin/python /home/intern/qfms/scripts/inference.py --device_type cuda --model_path /data/intern/llama2-7b/ --tokenizer /data/intern/llama2-7b/tokenizer.model --variant 7b --model_source hf
# TRITON_PRINT_AUTOTUNING=1 CUDA_VISIBLE_DEVICES=1 /home/intern/miniconda3/envs/test/bin/python /home/intern/qfms/scripts/inference.py --device_type cuda --model_path /data/intern/llama2-7b_q_int8_rot/ --tokenizer /data/intern/llama2-7b/tokenizer.model --variant 7b --quant_dtype int8 --rotate
CUDA_VISIBLE_DEVICES=1 /home/intern/miniconda3/envs/pt220/bin/python /home/intern/qfms/scripts/inference.py --device_type cuda --model_path /data/intern/llama2-7b_q_int4_rot_packed/ --tokenizer /data/intern/llama2-7b/tokenizer.model --variant 7b --quant_dtype int4-fake --rotate

# CUDA_VISIBLE_DEVICES=1 /home/intern/miniconda3/envs/myenv/bin/python /home/intern/qfms/scripts/inference.py --device_type cuda --model_path /data/intern/llama2-7b_q_int8_rot/ --tokenizer /data/intern/llama2-7b/tokenizer.model --variant 7b --quant_dtype int8 --rotate

# TRITON_PRINT_AUTOTUNING=1 CUDA_VISIBLE_DEVICES=0 /home/intern/miniconda3/envs/test/bin/python /home/intern/qfms/scripts/benchmark_inference.py --device_type cuda --tokenizer /data/intern/llama2-7b/tokenizer.model --variant 7b --quant_dtype int8 --rotate --skip_correctness_check --skip_nokvcache_runs --skip_e2e_runs --skip_eager_runs --compile_mode reduce-overhead
# TRITON_PRINT_AUTOTUNING=1 CUDA_VISIBLE_DEVICES=0 /home/intern/miniconda3/envs/test/bin/python /home/intern/qfms/scripts/benchmark_inference.py --device_type cuda --tokenizer /data/intern/llama2-7b/tokenizer.model --variant 7b --quant_dtype int8 --rotate --skip_correctness_check --skip_nokvcache_runs --skip_e2e_runs --skip_eager_runs